{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "failing-sandwich",
   "metadata": {},
   "source": [
    "# A First Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-centre",
   "metadata": {},
   "source": [
    "### Reviewing our Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-equilibrium",
   "metadata": {},
   "source": [
    "* inputs\n",
    "* weights and bias\n",
    "* fire or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-syndication",
   "metadata": {},
   "source": [
    "<img src=\"neuron-general-2.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-macedonia",
   "metadata": {},
   "source": [
    "We saw that this hypothesis function really consists of two components:\n",
    "1. Linear layer which can return any positive or negative or number, and\n",
    "2. Activation function which translates that output to a number between 1 and 0 (to represent firing or not).\n",
    "\n",
    "> Mathematically, we represented the neuron's linear function and activation function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-camping",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + b = w \\cdot x + b $\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-wealth",
   "metadata": {},
   "source": [
    "And we can represent this as code like so:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solar-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(x):\n",
    "    return w.dot(x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prerequisite-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    return 1/(1 + torch.exp(-z.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-draft",
   "metadata": {},
   "source": [
    "Then we can define some initial weights and a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classified-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight cell_area 2, weight for cell_concavities 1\n",
    "import torch\n",
    "w = torch.tensor([2., 1.])\n",
    "b = torch.tensor(-10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-queens",
   "metadata": {},
   "source": [
    "And pass through a vector $x$, which represents the efeeatures of an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "introductory-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# cell area is 3, and cell concavities is 4\n",
    "x = torch.tensor([2., 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-commander",
   "metadata": {},
   "source": [
    "So to get the out from the linear layer, we pass our data through the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "embedded-insulin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w.dot(x) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-northeast",
   "metadata": {},
   "source": [
    "And then pass that output to the activation layer, which returns a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "western-limitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1192)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "linear_layer(x):\n",
    "sigmoid_activation(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-arena",
   "metadata": {},
   "source": [
    "### Translating to a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "heard-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "disabled-passing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2., 4.])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "optimum-selection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8151], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "recognized-mexican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.5225, 0.0173]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "therapeutic-basic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3692], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "optimum-financing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6260,  0.0929]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)\n",
    "\n",
    "ll = nn.Linear(2, 1)\n",
    "\n",
    "ll.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-research",
   "metadata": {},
   "source": [
    "Neural network with just a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sexual-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([2., 4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "later-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prime-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-filename",
   "metadata": {},
   "source": [
    "* Take our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "collaborative-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell area is 3, and cell concavities is 4\n",
    "x = torch.tensor([2., 4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "consecutive-metropolitan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7962], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-moment",
   "metadata": {},
   "source": [
    "So $x$ represents the features of a single observation.  And we can see our neural network's predictions with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "challenging-bride",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0642], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-anderson",
   "metadata": {},
   "source": [
    "$z(x) = w \\cdot x + b$\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-liver",
   "metadata": {},
   "source": [
    "### Understanding the Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-windsor",
   "metadata": {},
   "source": [
    "Ok, so we just saw how we can create a neural network in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indian-warner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1), # length of each vectors, \n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-falls",
   "metadata": {},
   "source": [
    "* Breaking down the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-princess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-olive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "neural-millennium",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-liberal",
   "metadata": {},
   "source": [
    "###  Random Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "unusual-demand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6047,  0.2253]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = nn.Linear(2, 1)\n",
    "ll.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-niagara",
   "metadata": {},
   "source": [
    "* Seed manually to make sure start off with same weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moved-exhibit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4670, -0.5288]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "ll = nn.Linear(2, 1)\n",
    "\n",
    "ll.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-connectivity",
   "metadata": {},
   "source": [
    "### Creating multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-layout",
   "metadata": {},
   "source": [
    "* What does it mean to create multiple neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "owned-study",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2932, 0.2992]], requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(2, 1).weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dried-little",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2718,  0.6800],\n",
       "        [-0.6926, -0.0480],\n",
       "        [-0.0560,  0.5016]], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_3 = nn.Linear(2, 3) # 3 neurons of length 2\n",
    "\n",
    "ll_3.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "worth-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2., 4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "protected-basketball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1093, -1.3911,  1.8602], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-socket",
   "metadata": {},
   "source": [
    "Finally, the sigmoid function takes the output from our linear function and passes it through our sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "impossible-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "alternative-telephone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0688, 0.1676, 0.6037], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = ll_3(x)\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-habitat",
   "metadata": {},
   "source": [
    "And then our `nn.Sequential` function packages up our two functions, and passes the output from one layer into the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "psychological-summary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-senior",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-court",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to create a neural network -- with a single neuron -- in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "active-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-thumb",
   "metadata": {},
   "source": [
    "We saw that with the linear layer, we specify the number of input features, and the number of neurons -- where each neuron consists of a weight vector and a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "expanded-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.2977,  0.1892]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([-0.3397], requires_grad=True))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Linear(2, 1)\n",
    "layer._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-boston",
   "metadata": {},
   "source": [
    "And we saw that we can pass a feature vector to this layer, and it will apply the linear function $z(x) = w \\cdot x + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sealed-savannah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "magnetic-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1087], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-mexico",
   "metadata": {},
   "source": [
    "And finally that if we pass the feature vector to the neural net, that it will pass the feature vector through the linear layer, and that output to the sigmoid activation function to produce a prediction between 0 and 1 expressing the strength of the neuron firing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fatal-calculator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2172], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
