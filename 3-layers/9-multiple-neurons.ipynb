{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + b$ as: \n",
    "\n",
    "$z(x) =  \\begin{bmatrix}\n",
    "w_1 & w_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix} + b = (w \\cdot x + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# cell_area 2, cell_concavity 1\n",
    "x = torch.tensor([2, 1])\n",
    "             \n",
    "w = torch.tensor([3, 4])\n",
    "\n",
    "b = -8\n",
    "\n",
    "w.dot(x) + b\n",
    "# z = 3*2 + 4*1 - 8 = 2 -> neuron fires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* But we want more neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./artificial-network.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still with our **predicting cancer** example...\n",
    "\n",
    "\n",
    "* one neuron that determines if a cell is overly large,\n",
    "* and another neuron that determines if a cell has an abnormal shape.  \n",
    "\n",
    "* Sort of a separation of concerns between each neuron, them operating side by side in a layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* features of an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# area, perimeter, number concave points, symmetry error  \n",
    "x = torch.tensor([4, 2, 3, 2]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neuron for cells's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area, perimeter, number concave points, symmetry error  \n",
    "w_size = torch.tensor([2, 3, -.5, 0])\n",
    "\n",
    "b_size = -11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice that we still have weights for all of the features, even though focus is on size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(w_size) + b_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is our first neuron, and it is sensitive to changes in a neuron's size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neuron for shape \n",
    "* Our observation cell will still be the same, and so has the same four features with the same values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copy same observation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area, perimeter, number concave points, symmetry error  \n",
    "x = torch.tensor([4., 2., 3., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This time larger weights assocciated with shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area, perimeter, number concave points, symmetry error  \n",
    "w_shape = torch.tensor([0, -.5, 3, 1.5])\n",
    "\n",
    "b_shape = -14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(w_shape) + b_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways when using multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Different weights same input\n",
    "\n",
    "* use the **same feature vector** $x$ as an input to each neuron.  The two neurons simply weigh these inputs differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{size}(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "-.5 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} - 12 = .5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{shape}(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "3 \\\\\n",
    "1.5 \\\\\n",
    "\\end{bmatrix} - 14 = 1.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Or in Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice: the attributes of an observation $x_1$ through $x_4$, are fed to each of the (this time four, not two) neurons in the layer (each neuron is a white circle).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./first_layer_2.svg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Previously two separate weight vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_{size}(x) = \\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "-.5 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} = 12.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_{shape}(x) = \\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "3 \\\\\n",
    "1.5 \\\\\n",
    "\\end{bmatrix}= 8.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But can combine these into a *matrix*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "3 & -.5 \\\\\n",
    "-.5 & 3 \\\\\n",
    "0 & 1.5\\end{bmatrix} = \\begin{bmatrix}\n",
    "12.5 & 8.5 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more generally: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_{1}  & w_{2} \\\\\n",
    "|   & |\n",
    "\\end{bmatrix}  = \\begin{bmatrix}\n",
    "x \\cdot w_{1} & x \\cdot w_{2} \\end{bmatrix}  = \\begin{bmatrix} g_{1}(x) & g_{2}(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show this in code.  We'll start by placing our feature vectors in a weight matrix $W$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000],\n",
       "        [-0.5000,  3.0000],\n",
       "        [ 3.0000, -0.5000],\n",
       "        [ 1.5000,  0.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_size = torch.tensor([0., -.5, 3., 1.5])\n",
    "w_shape = torch.tensor([1., 3., -.5, 0.])\n",
    "\n",
    "# weight matrix\n",
    "W = torch.stack((w_size, w_shape), dim = 0).T\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.5000), tensor(12.5000))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ w_size, x @ w_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.5000, 12.5000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = x @ W\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **matrix:** A rectangular array of numbers arranged in rows and columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector, by contrast, consists of only one row or column, and it's size can be expressed with a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now above we've seen how we can calculate the weighted sum.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_{1}  & w_{2} \\\\\n",
    "|   & |\n",
    "\\end{bmatrix}  = \\begin{bmatrix}\n",
    "x \\cdot w_{1} & x \\cdot w_{2} \\end{bmatrix}  = \\begin{bmatrix} g_{1}(x) & g_{2}(x) \\end{bmatrix}$\n",
    "\n",
    "\n",
    "But we have still have not included the biases of our linear layer.  To complete our linear layer, we'll add a bias of $-12$ for $z_{size}$ and our bias of $-14$ for $z_{shape}$.  We do so by placing the two biases into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([-12, -7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we have a bias term for each neuron, and collect them into a bias vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then can calculate the output of passing an observation through a linear layer with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000,  1.5000])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or summarizing the above, we can calculate the outputs of both linear functions with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} z_1(x) & z_2(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can summarize as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = x\\cdot W  + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $z$ is a vector consisting of the output of each neuron's linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1.5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider that we have two different neurons that detect size and shape.  They do so with the following weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000],\n",
       "        [-0.5000,  3.0000],\n",
       "        [ 3.0000, -0.5000],\n",
       "        [ 1.5000,  0.0000]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2, 4, 3, 1])\n",
    "\n",
    "w_size = torch.tensor([2, 0, 1, 2])\n",
    "w_shape = torch.tensor([1, 1, 2, 0])\n",
    "\n",
    "W_new = torch.stack((w_size, w_shape), dim = 0).T\n",
    "W_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to calculate the output of $x \\cdot W$, where $x$ is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your answer with the dot product below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the sigmoid activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above, we saw the output from the linear layer by performing matrix vector multiplication like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000,  1.5000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "b = torch.tensor([-12, -7])\n",
    "\n",
    "w_size = torch.tensor([0, -.5, 3, 1.5])\n",
    "w_shape = torch.tensor([1, 3, -.5, 0])\n",
    "\n",
    "# weight matrix\n",
    "W = torch.stack((w_size, w_shape), dim = 0).T\n",
    "W\n",
    "\n",
    "z = x @ W + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, next we would pass these outputs to the sigmoid activation function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2689, 0.8176])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or mathematically, we write this entire hypothesis function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\sigma(x \\cdot W + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where sigma is applied to each entry of the vector resulting from $W\\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma (x \\cdot W + b) = \\begin{bmatrix} \\sigma(z_1) & \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or expressing the above formula as two layers of a neural network, our linear layer and an activation layer.  We can express this as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = (x \\cdot W + b)$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $z$ is our linear layer and $a$ is our activation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we saw the components to build a layer of a neural network.  A single layer is a combination of a weighted input and a sigmoid activation function.  \n",
    "\n",
    "The weighted input can be represented by $x \\cdot W + b$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} z_1(x) & z_2(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The row vector $x$ represents the features of a single observation.\n",
    "* Each column of the matrix W, contains the weights of a separate neuron, with the entries of $b$ as the corresponding biases.\n",
    "\n",
    "The output of the weighted input is fed into the activation function, which applies an entrywise operation.  Here, we use the sigmoid function.  So we can summarize the operations of our entire layer as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma (W\\cdot x + b) = \\begin{bmatrix} \\sigma(z_1) \\\\ \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can break up the above as a linear layer $z$ and an activation layer $a$ where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = (x \\cdot W + b)$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"./jigsaw-icon.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ W_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
