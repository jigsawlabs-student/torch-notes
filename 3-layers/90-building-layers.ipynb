{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, saw a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./first-layer.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = x\\cdot W  + b = \\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} z_1(x) & z_2(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A(z) = \\sigma (W\\cdot x + b) = \\begin{bmatrix} \\sigma(z_1) \\\\ \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll begin to see how we can take the outputs from our linear layer and activation layer, and feed this output into yet another layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./artificial-network.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is an information flow, where later layers make more abstract determination.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The rules of matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how data from one layer passes to another layer, really is understanding a math problem.  And to solve that math problem, we just need to be a little more familiar with matrix multiplication, and how by looking at one layer, we can predict what will pass to the next layer.\n",
    "\n",
    "\n",
    "\n",
    "Start back to a single layer, and two neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000],\n",
       "        [ 3.0000, -0.5000],\n",
       "        [-0.5000,  3.0000],\n",
       "        [ 0.0000,  1.5000]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w_size = torch.tensor([1, 3, -.5, 0])\n",
    "w_shape = torch.tensor([0, -.5, 3, 1.5])\n",
    "\n",
    "W = torch.stack((w_size, w_shape), dim = 0).T\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have our feature vector representing a single observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area, perimeter, number concave points, symmetry error  \n",
    "x_1 = torch.tensor([[2, 4, 3, 2]]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll can start by calculating the weighted sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(x) = x\\cdot W = \\begin{bmatrix}\n",
    "- & x_1 &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.5000, 10.0000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum = x_1 @ W \n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Focus on the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4]), torch.Size([4, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.shape, W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And then to sum a vector..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([-12, -7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 3.0000]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 @ W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we move onto the activation layer, by applying the sigmoid function, our dimensions stay the same.  This is because the sigmoid function applies to each entry: $z = x \\cdot W + b$\n",
    "\n",
    "$\\sigma (W\\cdot x + b) = \\begin{bmatrix} \\sigma(z_1) \\\\ \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 3.0000]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x_1 @ W + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6225, 0.9526]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matters because we'll need to build our multiple layers by passing the output from one layer as the input to another layer.  Let's see this visually, and then we'll try to better understand it through code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./big_layers.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Translate to code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From inputs to first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area, perimeter, number concave points, symmetry error, darkness, contrast\n",
    "x_2 = torch.tensor([[2, 4, 3, 2, 3, 5]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  1.5000,  0.1000],\n",
       "        [ 3.0000, -0.5000,  0.5000, -0.5000],\n",
       "        [-0.5000,  3.0000,  2.0000,  0.3000],\n",
       "        [ 0.0000,  1.5000,  1.5000,  0.0000],\n",
       "        [ 0.1000,  0.5000,  0.8000,  4.0000],\n",
       "        [ 1.3000,  1.0000,  1.5000, -2.0000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight Matrix 2\n",
    "import torch\n",
    "\n",
    "w_size = torch.tensor([1, 3, -.5, 0, .1, 1.3])\n",
    "w_shape = torch.tensor([0, -.5, 3., 1.5, .5, 1.])\n",
    "w_smoothness = torch.tensor([1.5, .5, 2, 1.5, .8, 1.5])\n",
    "w_color = torch.tensor([.1, -.5, .3, 0, 4, -2])\n",
    "\n",
    "W_1 = torch.stack((w_size, w_shape, w_smoothness, w_color), dim = 0).T\n",
    "W_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll need a bias vector of length 4, one for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -3, -12, -15,  -4])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_1 = torch.tensor([-3, -12, -15, -4])\n",
    "b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = None\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1 = None\n",
    "A_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From output of first layer to second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  1.5000],\n",
       "        [ 3.0000, -0.5000,  0.5000],\n",
       "        [-0.5000,  3.0000,  2.0000],\n",
       "        [ 0.0000,  1.5000,  1.5000]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_large_dark = torch.tensor([1, 3, -.5, 0])\n",
    "w_dark_irregular = torch.tensor([0, -.5, 3., 1.5])\n",
    "w_large_irregular = torch.tensor([1.5, .5, 2, 1.5])\n",
    "\n",
    "W_2 = torch.stack((w_large_dark, w_dark_irregular, w_large_irregular), dim = 0).T\n",
    "W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_2 = torch.tensor([-4, -5, -2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we take the output from the previous layer, $A_1$, and pass it to our linear layer, followed by our activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5329, -2.4167,  2.0725]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_2 = A_1 @ W_2 + b_2\n",
    "Z_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3698, 0.0819, 0.8882]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_2 = torch.sigmoid(Z_2)\n",
    "A_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making it Predictable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's see draw some conclusions about neural networks based on what we saw above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll use the image below as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./big_layers.svg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can feed it into a neural network that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "# fill in here\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2261, -0.3032, -0.9666]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two rules of matrix multiplication:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inner dimensions must be equal\n",
    "2. Outer dimensions determine the output of matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned how to build a neural network with multiple layers.  We can imagine that the earlier layers make more concrete assessments -- like assessing specific features, and pass these outputs to later layers to make more abstract assessements.   \n",
    "\n",
    "<img src=\"./big_layers.svg\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that under the hood, this works through matrix algebra.  With our example above, we saw a single observation with 6 features, multiplied  by the six weights of 4 neurons, results in an output from each neuron.  Then these four outputs are passed to the sigmoid layer, still resulting in four outputs.  Then these four outputs are each passed to the second layer, of three neurons, each with four weights.  The final output has three outputs.  We saw this with in Pytorch with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8253, -0.5326, -0.6160]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(4, 3)\n",
    ")\n",
    "\n",
    "x_1 # tensor([[2., 4., 3., 2., 3., 5.]])\n",
    "\n",
    "net(x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"./jigsaw-icon.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
