{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "meaning-professional",
   "metadata": {},
   "source": [
    "# The Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-movement",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "How does a neural network indicate a prediction of a 2 or 3, or a number 9?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-central",
   "metadata": {},
   "source": [
    "### Representing image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-thirty",
   "metadata": {},
   "source": [
    "Really what we're passing through is a list of pixels from an image.  Our image data will be black and white images of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-president",
   "metadata": {},
   "source": [
    "<img src=\"./mnist.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "novel-college",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "x = torch.randint(100, (1, 784))\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-delight",
   "metadata": {},
   "source": [
    "### The output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-upper",
   "metadata": {},
   "source": [
    "Our neural network will output the probability that the image could be any of the potential digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "municipal-claim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05, 0.62, 0.03, 0.03, 0.05, 0.03, 0.15, 0.01, 0.02, 0.01]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = [.05, .62, .03, .03, .05, .03, .15, .01, .02, .01]\n",
    "output\n",
    "# 0    1    2   3    4     5   6   7     8   9  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-february",
   "metadata": {},
   "source": [
    "So above,  \n",
    "* a $.05$ percent probability of the picture being a 0\n",
    "* a $.62$ probability of being a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-suite",
   "metadata": {},
   "source": [
    "### Defining our Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-entry",
   "metadata": {},
   "source": [
    "Now with this, we can define our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "standing-hearts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(5)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 64),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-helen",
   "metadata": {},
   "source": [
    "Let's make sure we understand what's going on under the hood by representing our neural network mathematically.  Mathematically, our our neural network is performing the following operation to produce the output of 10 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-prompt",
   "metadata": {},
   "source": [
    "$z_{1x64} = x_{1x784} \\cdot W_{784x64} + b_{1x64}$\n",
    "\n",
    "$a_{1x64} = \\sigma(Z_{1x64})$\n",
    "\n",
    "$z_{1x10} = a_{1x64} \\cdot W_{64x10} + b_{1x10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-advance",
   "metadata": {},
   "source": [
    "> Notice that the above follow our rules of matrix multiplication: (1) the inner numbers are equal and (2) the ouput of the matrix multiplication is determined by outer dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-above",
   "metadata": {},
   "source": [
    "And we can confirm that we get an output of tensor of length 10, by passing through our data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "maritime-democrat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-advantage",
   "metadata": {},
   "source": [
    "> Our feature vector starts of length 784, and when we pass it through our neural network, we get a vector of length 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accomplished-latex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0716,  0.1905,  0.2812, -0.0302,  0.4545, -0.0408,  1.0216, -0.1237,\n",
       "         -0.3579,  0.8472]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = net(x.float())\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-testing",
   "metadata": {},
   "source": [
    "### Taking the exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-tuition",
   "metadata": {},
   "source": [
    "Additional goals:\n",
    "\n",
    "1. Have one prediction dominate.\n",
    "* But because each potential output assigned some probability, this becomes difficult.  \n",
    "* So we want neural network to exaggerate prediction of most confident output.  \n",
    "\n",
    "\n",
    "* Do this with the exponent, which influences larger numbers a lot more than smaller numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brief-excuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3201)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = torch.tensor(1.2)\n",
    "\n",
    "torch.exp(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-pregnancy",
   "metadata": {},
   "source": [
    "> The above is called applying the exponential function.  So above, we do this by caclulating $e^{1.2} = 3.201$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-elevation",
   "metadata": {},
   "source": [
    "But for larger numbers like $4.8$, applying the exponent produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "broke-fairy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(121.5104)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "larger_num = torch.tensor(4.8)\n",
    "\n",
    "torch.exp(larger_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "defined-trace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0498)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_num = torch.tensor(-3.)\n",
    "\n",
    "torch.exp(neg_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-brighton",
   "metadata": {},
   "source": [
    "> We start with the following prediction from our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "considerable-kenya",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0716,  0.1905,  0.2812, -0.0302,  0.4545, -0.0408,  1.0216, -0.1237,\n",
       "         -0.3579,  0.8472]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-label",
   "metadata": {},
   "source": [
    "And applying the exponent produces the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "reserved-railway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9309, 1.2098, 1.3248, 0.9702, 1.5754, 0.9600, 2.7777, 0.8836, 0.6991,\n",
       "         2.3330]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = torch.exp(prediction)\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-turkey",
   "metadata": {},
   "source": [
    "So we can see that one number, $2.77$, tended to dominate many of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-drink",
   "metadata": {},
   "source": [
    "### Converting to probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-package",
   "metadata": {},
   "source": [
    "Now does our vector above look like a vector of probabilities? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "korean-terrace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9309, 1.2098, 1.3248, 0.9702, 1.5754, 0.9600, 2.7777, 0.8836, 0.6991,\n",
       "         2.3330]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-pickup",
   "metadata": {},
   "source": [
    "* But for valid probability, we need all of these numbers to add up to one.\n",
    "\n",
    "So we need to convert to percentages, by dividing by the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "turkish-rough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.6647, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_exp = torch.sum(exp)\n",
    "sum_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-strap",
   "metadata": {},
   "source": [
    "And then divide each entry by that total with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "disturbed-explorer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0681, 0.0885, 0.0969, 0.0710, 0.1153, 0.0703, 0.2033, 0.0647, 0.0512,\n",
       "         0.1707]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(exp/sum_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-deployment",
   "metadata": {},
   "source": [
    "### Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-eugene",
   "metadata": {},
   "source": [
    "**softmax function**: apply the exponent to each of our outputs, and then dividing by the sum of those exponents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "explicit-treatment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0681, 0.0885, 0.0969, 0.0710, 0.1153, 0.0703, 0.2033, 0.0647, 0.0512,\n",
       "         0.1707]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(prediction)/torch.sum(torch.exp(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-berry",
   "metadata": {},
   "source": [
    "Or, mathematically, it's the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-warren",
   "metadata": {},
   "source": [
    "$softmax(x) = \\frac{e^x}{\\sum e^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-breach",
   "metadata": {},
   "source": [
    "And we can tack it onto the end of our neural network, so that we get an output of probabilities, where one number tends to be dominant with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "collectible-solution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (3): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 64),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.Softmax(dim = 1)\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "caroline-master",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0681, 0.0885, 0.0969, 0.0710, 0.1153, 0.0703, 0.2033, 0.0647, 0.0512,\n",
       "         0.1707]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-germany",
   "metadata": {},
   "source": [
    "And if we want to represent this mathematically, this was our entire neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-candle",
   "metadata": {},
   "source": [
    "$z_{1x64} = x_{1x784} \\cdot W_{784x64} + b_{1x64}$\n",
    "\n",
    "$a_{1x64} = \\sigma(z_{1x64})$\n",
    "\n",
    "$z_{1x10} = A_{1x64} \\cdot W_{64x10} + b_{1x10}$\n",
    "\n",
    "$a_{1x10} = \\frac{e^z}{\\sum e^z}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-flash",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-battle",
   "metadata": {},
   "source": [
    "In this lesson, we wrapped up the prediction function for a neural network.  In doing so, we saw that we want our neural network to predict a different probability for each potential output.  So in predicting which of 10 digits to output, we'd want something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [.05, .62, .03, .03, .05, .03, .15, .01, .02, .01]\n",
    "output\n",
    "# 0    1    2   3    4     5   6   7     8   9  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-switzerland",
   "metadata": {},
   "source": [
    "To produce an output like the above, we first need our final linear layer to have a separate neuron for each output.  Then our next goal is to turn this output into a set of probabilities where one number tends to dominate.  To get one number to dominate, we apply the exponent to each entry, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "brief-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([[-0.0716,  0.1905,  0.2812, -0.0302,  0.4545, -0.0408,  1.0216, -0.1237,\n",
    "         -0.3579,  0.8472]])\n",
    "\n",
    "exp_output = torch.exp(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-haiti",
   "metadata": {},
   "source": [
    "Applying the exponent, exaggerates our larger numbers, increasing them by a lot -- while leaving the smaller numbers significantly less affected.  Finally, we need to turn our outputs into probabilities.  To do this, each entry must be between 0 and 1, and the entire set of outputs should add up to one.  It turns out we can achieve both goals, just by turning the above set of outputs into percentages, by dividing by the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "geological-decimal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0681, 0.0885, 0.0969, 0.0710, 0.1153, 0.0703, 0.2033, 0.0647, 0.0512,\n",
       "         0.1707]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_output/torch.sum(exp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-gibson",
   "metadata": {},
   "source": [
    "We saw that this procedure is called the softmax, and represented mathematically as: $softmax(x) = \\frac{e^x}{\\sum e^x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-nigeria",
   "metadata": {},
   "source": [
    "And, we represented all of the layers of our neural network with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-equity",
   "metadata": {},
   "source": [
    "$z_{1x64} = x_{1x784} \\cdot W_{784x64} + b_{1x64}$\n",
    "\n",
    "$a_{1x64} = \\sigma(Z_{1x64})$\n",
    "\n",
    "$z_{1x10} = A_{1x64} \\cdot W_{64x10} + b_{1x10}$\n",
    "\n",
    "$a_{1x10} = \\frac{e^z}{\\sum e^z}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-water",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"./jigsaw-icon.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
